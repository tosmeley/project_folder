{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNkCaHgB21DwbV5IBZTjkIJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tosmeley/project_folder/blob/main/week_13.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the TextVectorization layer"
      ],
      "metadata": {
        "id": "oZ9hF-ERpXJW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L7m0Rg-fAgly"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "class Vectorizer:\n",
        "  def standardize(self, text):\n",
        "    text = text.lower()\n",
        "    return \"\".join(char for char in text if char not in string.punctuation)\n",
        "\n",
        "  def tokenize(self, text):\n",
        "    text = self.standardize(text)\n",
        "    return text.split()\n",
        "\n",
        "  def make_vocabulary(self, dataset):\n",
        "    self.vocabulary = {\"\": 0, \"[UNK]\": 1}\n",
        "    for text in dataset:\n",
        "      text = self.standardize(text)\n",
        "      tokens = self.tokenize(text)\n",
        "      for token in tokens:\n",
        "        if token not in self.vocabulary:\n",
        "          self.vocabulary[token] = len(self.vocabulary)\n",
        "    \n",
        "    self.inverse_vocabulary = dict((v, k) for k, v in self.vocabulary.items())\n",
        "\n",
        "  def encode(self, text):\n",
        "    text = self.standardize(text)\n",
        "    tokens = self.tokenize(text)\n",
        "    return [self.vocabulary.get(token, 1) for token in tokens]\n",
        "\n",
        "  def decode(self, int_sequence):\n",
        "    return \" \".join(self.inverse_vocabulary.get(i, \"[UNK]\") for i in int_sequence)\n",
        "\n",
        "vectorizer = Vectorizer()\n",
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "vectorizer.make_vocabulary(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = vectorizer.encode(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SMq-WymHcb3",
        "outputId": "501749a6-df70-4c5f-a760-2c3b15f594bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 3, 5, 7, 1, 5, 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_sentence = vectorizer.decode(encoded_sentence)\n",
        "print(decoded_sentence)"
      ],
      "metadata": {
        "id": "jaOB2zonIulB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2cc0108-8124-4dee-ed76-e3999ca8d7e9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "i write rewrite and [UNK] rewrite again\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode = \"int\",\n",
        ")"
      ],
      "metadata": {
        "id": "SxF-K8yLJWxi"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import tensorflow as tf\n",
        "\n",
        "def custom_standardization_fn(string_tensor):\n",
        "  lowercase_string = tf.strings.lower(string_tensor)\n",
        "  return tf.strings.regex_replace(lowercase_string, f\"[{re.escape(string.punctuation)}]\", \"\")\n",
        "\n",
        "def custom_split_fn(string_tensor):\n",
        "  return tf.strings.split(string_tensor)\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    output_mode = \"int\",\n",
        "    standardize = custom_standardization_fn,\n",
        "    split = custom_split_fn,\n",
        ")"
      ],
      "metadata": {
        "id": "4Myg-oZapoL_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "    \"I write, erase, rewrite\",\n",
        "    \"Erase again, and then\",\n",
        "    \"A poppy blooms.\",\n",
        "]\n",
        "text_vectorization.adapt(dataset)"
      ],
      "metadata": {
        "id": "ue9CpERmptSb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the vocabulary\n",
        "\n"
      ],
      "metadata": {
        "id": "FYQbjWESpxw6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.get_vocabulary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGA0sAp1puy8",
        "outputId": "31ef3ee2-fa3e-4c5f-b7b0-87c09193fbce"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['',\n",
              " '[UNK]',\n",
              " 'erase',\n",
              " 'write',\n",
              " 'then',\n",
              " 'rewrite',\n",
              " 'poppy',\n",
              " 'i',\n",
              " 'blooms',\n",
              " 'and',\n",
              " 'again',\n",
              " 'a']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "test_sentence = \"I write, rewrite, and still rewrite again\"\n",
        "encoded_sentence = text_vectorization(test_sentence)\n",
        "print(encoded_sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vw7BF8MQp1cH",
        "outputId": "f6c5debd-b57e-4a7e-a90a-8908102015dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Two approaches for representing groups of words: Sets and sequences\n",
        "\n",
        "Preparing the IMDB movie reviews data\n"
      ],
      "metadata": {
        "id": "8AMfEkhSp6dH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpdoCE29p91c",
        "outputId": "8465ce27-764f-48c5-d435-2c505ef29d4f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  13.7M      0  0:00:05  0:00:05 --:--:-- 17.4M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -r aclImdb/train/unsup"
      ],
      "metadata": {
        "id": "U7Ha3FE8qGCa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cat aclImdb/train/pos/4077_10.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUx-jqcGqIMr",
        "outputId": "797d87ff-79a7-46f3-f4ee-a0506a3d4d2f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "for category in (\"neg\", \"pos\"):\n",
        "  os.makedirs(val_dir / category)\n",
        "  files = os.listdir(train_dir / category)\n",
        "  random.Random(1337).shuffle(files)\n",
        "  num_val_samples = int(0.2 * len(files))\n",
        "  val_files = files[-num_val_samples:]\n",
        "  for fname in val_files:\n",
        "    shutil.move(train_dir / category / fname, val_dir / category / fname)"
      ],
      "metadata": {
        "id": "GI5zJnOeqKGU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size = batch_size\n",
        ")\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size = batch_size\n",
        ")\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size = batch_size\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnfick-cqMep",
        "outputId": "3993fe28-a89a-4115-bf13-8640dc2defb6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Displaying the shapes and dtypes of the first batch\n",
        "\n"
      ],
      "metadata": {
        "id": "AVOe2kJ0qQZJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]\", inputs[0])\n",
        "  print(\"targets[0]\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zUpvshH4qV9e",
        "outputId": "5542d728-d7e3-4d73-f23c-176865cf7206"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0] tf.Tensor(b\"The unlikely duo of Zero Mostel and Harry Belafonte team up to give us some interesting performances and subject matter in The Angel Levine. It's one interesting twist on the themes from It's A Wonderful Life.<br /><br />Zero is married to Ida Kaminsky and the two of them belong to a special class of elderly Jewish poor in New York. Mostel used to be a tailor and proud of his trade, but his back and arthritis have prevented him from working. Kaminsky is mostly bedridden. He's reduced to applying for welfare. In desperation like Jimmy Stewart, he cries out to God for some help.<br /><br />Now maybe if he had gotten someone like Henry Travers things might have worked out differently, but even Stewart had trouble accepting Travers. But Travers had one thing going for him, he was over 100 years off this mortal coil and all his ties to earthly things were gone. God sent Mostel something quite different, the recently deceased Harry Belafonte who should have at least been given some basic training for angels before being given an assignment.<br /><br />Belafonte hasn't accepted he's moved on from life, he's still got a lot of issues. He also has a wife, Gloria Foster, who doesn't know he's passed on, hit by a car right at the beginning of the film. You put his issues and Mostel's issues and you've got a good conflict, starting with the fact that Mostel can't believe in a black Jew named Levine.<br /><br />This was the farewell performance for Polish/Jewish actress Ida Kaminsky who got a nomination for Best Actress in The Shop on Main Street a few years back. The other prominent role here is that of Irish actor Milo O'Shea playing a nice Jewish doctor. Remembering O'Shea's brogue from The Verdict, I was really surprised to see and hear him carry off the part of the doctor.<br /><br />The Angel Levine raises some interesting and disturbing questions about faith and race in this society. It's brought to you by a stellar cast and of course created by acclaimed writer Bernard Malamud. Make sure to catch it when broadcast.\", shape=(), dtype=string)\n",
            "targets[0] tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Processing words as a set: The bag-of-words approach\n",
        "\n",
        "Single words (unigrams) with binary encoding\n",
        "\n",
        "Preprocessing our datasets with a **TextVectorization** layer\n",
        "\n"
      ],
      "metadata": {
        "id": "iWmlUaQCqoPB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"multi_hot\",\n",
        ")\n",
        "\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_1gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        ")\n",
        "binary_1gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        ")\n",
        "binary_1gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        ")"
      ],
      "metadata": {
        "id": "ZWYXyHIOqt_z"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspecting the output of our binary unigram dataset\n",
        "\n"
      ],
      "metadata": {
        "id": "JS5FPwPIqy5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "  print(\"inputs.shape:\", inputs.shape)\n",
        "  print(\"inputs.dtype:\", inputs.dtype)\n",
        "  print(\"targets.shape:\", targets.shape)\n",
        "  print(\"targets.dtype:\", targets.dtype)\n",
        "  print(\"inputs[0]\", inputs[0])\n",
        "  print(\"targets[0]\", targets[0])\n",
        "  break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUjB7mKiq0P3",
        "outputId": "86fb354c-a699-4105-a898-98d51105f16a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0] tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0] tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model-building utility\n",
        "\n"
      ],
      "metadata": {
        "id": "bDZ2gZAMq4os"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens = 20000, hidden_dim = 16):\n",
        "  inputs = keras.Input(shape = (max_tokens,))\n",
        "  x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "  x = layers.Dropout(0.5)(x)\n",
        "  outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "  model = keras.Model(inputs, outputs)\n",
        "  model.compile(optimizer=\"rmsprop\",\n",
        "                loss=\"binary_crossentropy\",\n",
        "                metrics=[\"accuracy\"])\n",
        "  return model"
      ],
      "metadata": {
        "id": "VtDTEyV9q2HP"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing the binary unigram model\n",
        "\n"
      ],
      "metadata": {
        "id": "bDXFWiWArBIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\", save_best_only = True)\n",
        "]\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data = binary_1gram_val_ds.cache(),\n",
        "          epochs = 10,\n",
        "          callbacks = callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uyAf66Oaq_I2",
        "outputId": "bc9d3559-8d64-434d-f900-72a5f03f2585"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 22s 33ms/step - loss: 0.4281 - accuracy: 0.8197 - val_loss: 0.2986 - val_accuracy: 0.8800\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2819 - accuracy: 0.8977 - val_loss: 0.2887 - val_accuracy: 0.8874\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2540 - accuracy: 0.9100 - val_loss: 0.2999 - val_accuracy: 0.8866\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 6s 10ms/step - loss: 0.2336 - accuracy: 0.9219 - val_loss: 0.3112 - val_accuracy: 0.8864\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2300 - accuracy: 0.9259 - val_loss: 0.3281 - val_accuracy: 0.8882\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2222 - accuracy: 0.9276 - val_loss: 0.3339 - val_accuracy: 0.8868\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 6s 9ms/step - loss: 0.2182 - accuracy: 0.9288 - val_loss: 0.3411 - val_accuracy: 0.8868\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2179 - accuracy: 0.9327 - val_loss: 0.3456 - val_accuracy: 0.8864\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.2209 - accuracy: 0.9331 - val_loss: 0.3493 - val_accuracy: 0.8872\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2111 - accuracy: 0.9337 - val_loss: 0.3640 - val_accuracy: 0.8794\n",
            "782/782 [==============================] - 9s 11ms/step - loss: 0.2961 - accuracy: 0.8839\n",
            "Test acc: 0.884\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigrams with binary encoding\n",
        "\n",
        "Configuring the **TextVectorization** layer to return bigrams\n",
        "\n"
      ],
      "metadata": {
        "id": "WBt8UlY8rgI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"multi_hot\",\n",
        ")"
      ],
      "metadata": {
        "id": "tbZuUru0re-4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing the binary bigram model\n",
        "\n"
      ],
      "metadata": {
        "id": "-VG3IvOJrqWE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        "    )\n",
        "binary_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        "    )\n",
        "binary_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4\n",
        "    )\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\", save_best_only=True)\n",
        "]\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data = binary_2gram_val_ds.cache(),\n",
        "          epochs = 10,\n",
        "          callbacks = callbacks)\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SGx3K3k9roBy",
        "outputId": "a141ed32-6270-4636-e5e5-20ece7e63367"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_2 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 20ms/step - loss: 0.3856 - accuracy: 0.8396 - val_loss: 0.2763 - val_accuracy: 0.8896\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2493 - accuracy: 0.9104 - val_loss: 0.2789 - val_accuracy: 0.8966\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 5s 7ms/step - loss: 0.2138 - accuracy: 0.9299 - val_loss: 0.2929 - val_accuracy: 0.8986\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1977 - accuracy: 0.9383 - val_loss: 0.3103 - val_accuracy: 0.8964\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1973 - accuracy: 0.9430 - val_loss: 0.3252 - val_accuracy: 0.8932\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1858 - accuracy: 0.9468 - val_loss: 0.3367 - val_accuracy: 0.8938\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1890 - accuracy: 0.9488 - val_loss: 0.3453 - val_accuracy: 0.8902\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.1756 - accuracy: 0.9497 - val_loss: 0.3561 - val_accuracy: 0.8904\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.1711 - accuracy: 0.9509 - val_loss: 0.3674 - val_accuracy: 0.8894\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 5s 8ms/step - loss: 0.1838 - accuracy: 0.9499 - val_loss: 0.3704 - val_accuracy: 0.8892\n",
            "782/782 [==============================] - 13s 16ms/step - loss: 0.2730 - accuracy: 0.8956\n",
            "Test acc: 0.896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigrams with TF-IDF encoding\n",
        "\n",
        "Configuring the **TextVectorization** layer to return token counts"
      ],
      "metadata": {
        "id": "SFVoERR7sK-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"count\",\n",
        ")"
      ],
      "metadata": {
        "id": "6hix6YAZrte_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Configuring **TextVectorization** to return TF-IDF-weighted outputs\n",
        "\n"
      ],
      "metadata": {
        "id": "axr6G05xsSI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams = 2,\n",
        "    max_tokens = 20000,\n",
        "    output_mode = \"tf_idf\",\n",
        ")"
      ],
      "metadata": {
        "id": "Wx5fJO8VsPKn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training and testing the TF-IDF bigram model\n",
        "\n"
      ],
      "metadata": {
        "id": "Vw0jlHCcsZYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4)\n",
        "tfidf_2gram_val_ds = val_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4)\n",
        "tfidf_2gram_test_ds = test_ds.map(\n",
        "    lambda x, y: (text_vectorization(x), y),\n",
        "    num_parallel_calls = 4)\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\", save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data = tfidf_2gram_val_ds.cache(),\n",
        "          epochs = 10,\n",
        "          callbacks = callbacks)\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2S1vhnSsWP2",
        "outputId": "4cff9eb9-dc74-4d67-b1e3-18e3328f8c7f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 13s 20ms/step - loss: 0.5016 - accuracy: 0.7876 - val_loss: 0.3112 - val_accuracy: 0.8790\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.3197 - accuracy: 0.8727 - val_loss: 0.3105 - val_accuracy: 0.8794\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2880 - accuracy: 0.8916 - val_loss: 0.3089 - val_accuracy: 0.8918\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2568 - accuracy: 0.9016 - val_loss: 0.3473 - val_accuracy: 0.8834\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2372 - accuracy: 0.9057 - val_loss: 0.3535 - val_accuracy: 0.8790\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2431 - accuracy: 0.9085 - val_loss: 0.3489 - val_accuracy: 0.8856\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2302 - accuracy: 0.9107 - val_loss: 0.3659 - val_accuracy: 0.8778\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2240 - accuracy: 0.9115 - val_loss: 0.3587 - val_accuracy: 0.8696\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2236 - accuracy: 0.9103 - val_loss: 0.3841 - val_accuracy: 0.8762\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 4s 7ms/step - loss: 0.2194 - accuracy: 0.9122 - val_loss: 0.3788 - val_accuracy: 0.8734\n",
            "782/782 [==============================] - 10s 11ms/step - loss: 0.3061 - accuracy: 0.8960\n",
            "Test acc: 0.896\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape = (1,), dtype = \"string\")\n",
        "processed_inputs = text_vectorization(inputs)\n",
        "outputs = model(processed_inputs)\n",
        "inference_model = keras.Model(inputs, outputs)"
      ],
      "metadata": {
        "id": "7CdoVyULsdW5"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "raw_text_data = tf.convert_to_tensor([\n",
        "    [\"That was an excellent movie, I loved it.\"],\n",
        "])\n",
        "predictions = inference_model(raw_text_data)\n",
        "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qdRBFXVasfnd",
        "outputId": "d5a28ba8-efa9-4296-c748-11b3b83eb3a0"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "99.43 percent positive\n"
          ]
        }
      ]
    }
  ]
}